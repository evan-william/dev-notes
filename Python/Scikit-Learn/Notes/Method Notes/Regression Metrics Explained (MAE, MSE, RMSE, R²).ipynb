{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c9d292-5bd1-4620-b1fb-a804450c9ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.750\n",
      "MSE:  0.875\n",
      "RMSE: 0.935\n",
      "RÂ²:   0.724\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# ðŸ“Š Regression Metrics Explained (MAE, MSE, RMSE, RÂ²)\n",
    "# ==========================================================\n",
    "# These metrics are used to evaluate how good our regression\n",
    "# model is at predicting continuous values (like house price,\n",
    "# temperature, salary, etc.).\n",
    "#\n",
    "# They each measure \"error\" differently, and knowing when to\n",
    "# use which metric is very important in Machine Learning.\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Example \"true values\" (what actually happened)\n",
    "y_true = np.array([3.0, 5.0, 2.5, 7.0])\n",
    "\n",
    "# Example \"predicted values\" (what the model guessed)\n",
    "y_pred = np.array([2.5, 5.0, 4.0, 8.0])\n",
    "\n",
    "# -------------------------------\n",
    "# 1) MAE - Mean Absolute Error\n",
    "# -------------------------------\n",
    "# Formula: MAE = (1/n) * Î£ |y_i - y_pred_i|\n",
    "#\n",
    "# Intuition:\n",
    "# - Take the difference between true and predicted values.\n",
    "# - Make them positive (absolute value).\n",
    "# - Average them all.\n",
    "#\n",
    "# MAE tells us: \"On average, how far off are we?\"\n",
    "# Example: If MAE = 5000 in house prices, the model\n",
    "#          is off by $5000 on average.\n",
    "#\n",
    "# Pros: Easy to interpret, same units as target.\n",
    "# Cons: Treats all errors equally (doesn't punish big mistakes more).\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# -------------------------------\n",
    "# 2) MSE & RMSE\n",
    "# -------------------------------\n",
    "# Formula: MSE = (1/n) * Î£ (y_i - y_pred_i)^2\n",
    "#          RMSE = sqrt(MSE)\n",
    "#\n",
    "# Intuition:\n",
    "# - Like MAE, but square the errors first.\n",
    "# - Squaring makes large errors MUCH larger.\n",
    "# - Then average them (MSE).\n",
    "# - Finally, take square root to bring it back to original units (RMSE).\n",
    "#\n",
    "# RMSE tells us: \"How far off are we, with big mistakes\n",
    "#                punished more harshly?\"\n",
    "# Example: If predicting flight delays, and being\n",
    "#          1 hour off is much worse than being 5 minutes off,\n",
    "#          RMSE is more useful than MAE.\n",
    "#\n",
    "# Pros: Penalizes large errors (useful when big errors cost a lot).\n",
    "# Cons: Sensitive to outliers (one huge error can dominate RMSE).\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# -------------------------------\n",
    "# 3) RÂ² - Coefficient of Determination\n",
    "# -------------------------------\n",
    "# Formula: RÂ² = 1 - (SS_res / SS_tot)\n",
    "#   - SS_res = Î£ (y_i - y_pred_i)^2   -> residual sum of squares (error left over)\n",
    "#   - SS_tot = Î£ (y_i - y_mean)^2     -> total sum of squares (variance in data)\n",
    "#\n",
    "# Intuition:\n",
    "# - RÂ² measures how much of the variation in the target\n",
    "#   is explained by the model compared to just guessing the mean.\n",
    "#\n",
    "# Interpretation:\n",
    "#   - RÂ² = 1.0 â†’ perfect prediction\n",
    "#   - RÂ² = 0.0 â†’ model no better than just predicting the average\n",
    "#   - RÂ² < 0  â†’ model is worse than predicting the average (bad!)\n",
    "#\n",
    "# Pros: Easy to compare across models (unitless measure).\n",
    "# Cons: Can be misleading alone (high RÂ² â‰  good predictions).\n",
    "#       Always combine with MAE/RMSE.\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”Ž Summary of Metrics\n",
    "# -------------------------------\n",
    "# MAE  â†’ \"Average absolute error\" (easy to explain, robust).\n",
    "# RMSE â†’ \"Like MAE but punishes large errors more\" (good if large errors are costly).\n",
    "# RÂ²   â†’ \"Proportion of variance explained\" (good overall fit measure, but not enough alone).\n",
    "#\n",
    "# Best Practice:\n",
    "# âœ… Report at least MAE, RMSE, and RÂ² together.\n",
    "# âœ… Use residual plots (errors vs predictions) to see if errors are random or biased.\n",
    "# âœ… For fair comparison, use cross-validation (not just 1 test split).\n",
    "\n",
    "# -------------------------------\n",
    "# Print Results\n",
    "# -------------------------------\n",
    "print(f\"MAE:  {mae:.3f}\")\n",
    "print(f\"MSE:  {mse:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"RÂ²:   {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91881842-5227-4e2d-bd4e-4a9555e1e25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
