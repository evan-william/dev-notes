{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53182456-4324-4bfb-9f10-0ee4037bf84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch_openml → to download datasets from OpenML (example: MNIST digits)\n",
    "# We'll often use this to get high-dimensional data where PCA is useful.\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# PCA (Principal Component Analysis) → dimensionality reduction technique\n",
    "# It helps to reduce features while keeping most of the variance in the data.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# LogisticRegression → a classifier (supervised ML model)\n",
    "# We’ll later test how PCA-transformed features affect classification accuracy.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# train_test_split → to split dataset into training and testing sets\n",
    "# Important for evaluating how well the model generalizes.\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f8e4aa-b1b9-4ffa-ab9a-ae0fa34f8446",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', return_X_y = True)\n",
    "\"\"\"\n",
    "# 'mnist_784' → standard name for MNIST dataset with 784 features.\n",
    "# Why 784?\n",
    "# - Each MNIST image is 28 x 28 pixels = 784 pixels total.\n",
    "# - Each pixel is stored as one feature (value 0–255 for grayscale intensity).\n",
    "# - So every row in X is a flattened image of length 784.\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd3502f-da85-43b5-9a31-496cd08bf443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data shape: (56000, 784)\n",
      "Reduced training data shape: (56000, 10)\n",
      "Original test data shape: (14000, 784)\n",
      "Reduced test data shape: (14000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Create a PCA object\n",
    "# -------------------\n",
    "# n_components=10 → we want to keep only 10 principal components (features).\n",
    "# - The original data has 784 features (28x28 pixels per image).\n",
    "# - PCA compresses these 784 features into 10 while retaining as much variance as possible.\n",
    "# - This is useful to speed up training and reduce noise.\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "# Fit PCA on the training data and transform it\n",
    "# ---------------------------------------------\n",
    "# - pca.fit_transform(X_train) learns the principal components from the training set\n",
    "#   and applies the dimensionality reduction.\n",
    "# - Result: X_train_reduced → same number of rows (images) but fewer columns (features).\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformation to the test data\n",
    "# ----------------------------------------------\n",
    "# - Important: We only \"fit\" PCA on the training data.\n",
    "# - Then we \"transform\" test data using the same transformatin,\n",
    "#   so the model doesn’t \"cheat\" by learning from the test set\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "# Print shapes to compare before vs after PCA\n",
    "print(\"Original training data shape:\", X_train.shape)\n",
    "# → (n_samples, 784)  because each image is 28x28 pixels flattened\n",
    "\n",
    "print(\"Reduced training data shape:\", X_train_reduced.shape)\n",
    "# → (n_samples, 10)   because we kept only 10 principal components\n",
    "\n",
    "print(\"Original test data shape:\", X_test.shape)\n",
    "print(\"Reduced test data shape:\", X_test_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9942666e-ffaf-4951-89b1-49628ee8f02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Evan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (original 784 features): 0.9207142857142857\n",
      "Logistic Regression (PCA-reduced 10 features): 0.7993571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Evan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nExplanation:\\n------------\\n- acc_full    → accuracy when model sees ALL 784 pixel values.\\n- acc_reduced → accuracy when model only sees 10 compressed PCA features.\\n\\nKey takeaway:\\n- Full features = higher accuracy (more info).\\n- PCA features = faster training, smaller data, but slight accuracy drop.\\n- PCA is a trade-off between efficiency and performance.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell: Compare Logistic Regression with and without PCA\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# --- Logistic Regression on Original Data (784 features) ---\n",
    "clf_full = LogisticRegression(max_iter=100)   # model using all pixels\n",
    "clf_full.fit(X_train, y_train)                # fit on raw features\n",
    "acc_full = clf_full.score(X_test, y_test)     # evaluate on test data\n",
    "\n",
    "# --- Logistic Regression on PCA-Reduced Data (10 features) ---\n",
    "clf_reduced = LogisticRegression(max_iter=100)   # model using compressed features\n",
    "clf_reduced.fit(X_train_reduced, y_train)        # fit on PCA-transformed data\n",
    "acc_reduced = clf_reduced.score(X_test_reduced, y_test)   # evaluate on reduced test data\n",
    "\n",
    "# --- Results ---\n",
    "print(\"Logistic Regression (original 784 features):\", acc_full)\n",
    "print(\"Logistic Regression (PCA-reduced 10 features):\", acc_reduced)\n",
    "\n",
    "\"\"\"\n",
    "Explanation:\n",
    "------------\n",
    "- acc_full    → accuracy when model sees ALL 784 pixel values.\n",
    "- acc_reduced → accuracy when model only sees 10 compressed PCA features.\n",
    "\n",
    "Key takeaway:\n",
    "- Full features = higher accuracy (more info).\n",
    "- PCA features = faster training, smaller data, but slight accuracy drop.\n",
    "- PCA is a trade-off between efficiency and performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3b77a63-b722-44f9-87eb-07d55a2dfce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio per component:\n",
      " [0.0974149  0.07158571 0.06143781 0.05414128 0.0488586  0.04299859\n",
      " 0.03284287 0.02906657 0.02762179 0.0233081 ]\n",
      "\n",
      "Total variance retained (sum): 0.48927621135911376\n"
     ]
    }
   ],
   "source": [
    "# PCA Variance Explained\n",
    "import numpy as np\n",
    "print(\"Explained variance ratio per component:\\n\", pca.explained_variance_ratio_)\n",
    "\n",
    "print(\"\\nTotal variance retained (sum):\", np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b36b90-6376-4385-8c83-9d5bf5f17676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
